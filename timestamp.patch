diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 32ae3aa50c7e..4d99c8521381 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -938,6 +938,13 @@ struct kvm_vcpu_arch {
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
 	u64 l1_tsc_scaling_ratio;
+	/* For custom RDTSC/RDTSCP emulation */
+    u64 guest_tsc_accumulator;          	/* Stores the current emulated guest TSC value */
+    u64 previous_host_tsc_for_scaling;  	/* Host TSC at the last RDTSC/RDTSCP emulation */
+    u64 initial_guest_tsc_offset;       	/* Large random offset for guest TSC initialization */
+    u32 host_calibrated_khz;            	/* Host TSC frequency in KHz (from tsc_khz) */
+    u32 guest_target_khz;               	/* Desired guest TSC frequency in KHz */
+    u64 last_rdtsc_handler_exit_host_tsc; 	/* Host TSC at *exit* of the last RDTSC/RDTSCP handler */
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 9b92f3f56f49..a9b65901b468 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -2030,18 +2030,32 @@ EXPORT_SYMBOL_GPL(kvm_cpuid);
 
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
-	u32 eax, ebx, ecx, edx;
+    u32 guest_eax_input, guest_ecx_input;
+    u32 result_eax, result_ebx, result_ecx, result_edx;
 
-	if (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))
-		return 1;
+    if (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))
+        return 1;
+
+    guest_eax_input = kvm_rax_read(vcpu);
+    guest_ecx_input = kvm_rcx_read(vcpu);
+
+    result_eax = guest_eax_input;
+    result_ecx = guest_ecx_input;
+
+    kvm_cpuid(vcpu, &result_eax, &result_ebx, &result_ecx, &result_edx, false);
 
-	eax = kvm_rax_read(vcpu);
-	ecx = kvm_rcx_read(vcpu);
-	kvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);
-	kvm_rax_write(vcpu, eax);
-	kvm_rbx_write(vcpu, ebx);
-	kvm_rcx_write(vcpu, ecx);
-	kvm_rdx_write(vcpu, edx);
-	return kvm_skip_emulated_instruction(vcpu);
+    // 0x40000003 on EBX indicates the flags that a parent partition specified to create a child partition (https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/tlfs/datatypes/hv_partition_privilege_mask)
+    if (guest_eax_input == 0x40000003) {
+        // Ensure this bit is set to 1 (AccessPartitionReferenceCounter).
+        result_ebx |= 0x00000001; // Set bit 0 of result_ebx to 1 using a bitwise OR operation.
+    }
+
+    kvm_rax_write(vcpu, result_eax);
+    kvm_rbx_write(vcpu, result_ebx);
+    kvm_rcx_write(vcpu, result_ecx);
+    kvm_rdx_write(vcpu, result_edx);
+
+    return kvm_skip_emulated_instruction(vcpu);
 }
+
 EXPORT_SYMBOL_GPL(kvm_emulate_cpuid);
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 3b92f893b239..549f54845af6 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -29,6 +29,10 @@
 #include <linux/tboot.h>
 #include <linux/trace_events.h>
 #include <linux/entry-kvm.h>
+#include <linux/timekeeping.h>
+#include <linux/math64.h>
+#include <linux/random.h>
+#include <asm/tsc.h>
 
 #include <asm/apic.h>
 #include <asm/asm.h>
@@ -70,6 +74,7 @@
 #include "x86.h"
 #include "x86_ops.h"
 #include "smm.h"
+
 #include "vmx_onhyperv.h"
 #include "posted_intr.h"
 
@@ -4492,10 +4497,12 @@ static u32 vmx_exec_control(struct vcpu_vmx *vmx)
 	 * Not used by KVM, but fully supported for nesting, i.e. are allowed in
 	 * vmcs12 and propagated to vmcs02 when set in vmcs12.
 	 */
-	exec_control &= ~(CPU_BASED_RDTSC_EXITING |
-			  CPU_BASED_USE_IO_BITMAPS |
+	exec_control &= ~(CPU_BASED_USE_IO_BITMAPS |
 			  CPU_BASED_MONITOR_TRAP_FLAG |
 			  CPU_BASED_PAUSE_EXITING);
+	
+	// Ensure handle_rdtsc() is used.
+	exec_control |= CPU_BASED_RDTSC_EXITING;
 
 	/* INTR_WINDOW_EXITING and NMI_WINDOW_EXITING are toggled dynamically */
 	exec_control &= ~(CPU_BASED_INTR_WINDOW_EXITING |
@@ -4892,6 +4899,21 @@ void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	vmx->msr_ia32_umwait_control = 0;
 
+    vcpu->arch.guest_target_khz = 2800000;
+
+    if (tsc_khz == 0) {
+        printk(KERN_WARNING "KVM: host_calibrated_khz is zero in vmx_vcpu_reset. TSC scaling may be incorrect.\n");
+        vcpu->arch.host_calibrated_khz = 1;
+    } else {
+        vcpu->arch.host_calibrated_khz = tsc_khz;
+    }
+
+    vcpu->arch.initial_guest_tsc_offset = get_random_u64();
+    vcpu->arch.guest_tsc_accumulator = vcpu->arch.initial_guest_tsc_offset;
+    
+    vcpu->arch.previous_host_tsc_for_scaling = rdtsc(); 
+    vcpu->arch.last_rdtsc_handler_exit_host_tsc = 0;
+	
 	vmx->hv_deadline_tsc = -1;
 	kvm_set_cr8(vcpu, 0);
 
@@ -6113,6 +6135,100 @@ static int handle_notify(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+// Use only for debug purposes, otherwise VM will be KINDA KINDA slow
+static u32 print_once = 0;
+
+#define HOST_CYCLES_TIMING_TEST_THRESHOLD 10000
+
+static u64 get_scaled_guest_tsc(struct kvm_vcpu *vcpu) {
+    u64 current_host_tsc_at_entry;
+    u64 delta_host_tsc;
+    u64 guest_tsc_increment;
+    u64 host_tsc_since_last_rdtsc_handler_exit;
+
+    current_host_tsc_at_entry = rdtsc();
+
+    if (likely(vcpu->arch.last_rdtsc_handler_exit_host_tsc != 0 &&
+               current_host_tsc_at_entry > vcpu->arch.last_rdtsc_handler_exit_host_tsc)) {
+        host_tsc_since_last_rdtsc_handler_exit = current_host_tsc_at_entry - vcpu->arch.last_rdtsc_handler_exit_host_tsc;
+    } else if (vcpu->arch.last_rdtsc_handler_exit_host_tsc == 0) {
+        host_tsc_since_last_rdtsc_handler_exit = ULLONG_MAX;
+    } else {
+        host_tsc_since_last_rdtsc_handler_exit = 0;
+    }
+
+    if (likely(current_host_tsc_at_entry > vcpu->arch.previous_host_tsc_for_scaling)) {
+        delta_host_tsc = current_host_tsc_at_entry - vcpu->arch.previous_host_tsc_for_scaling;
+    } else {
+        delta_host_tsc = (current_host_tsc_at_entry == vcpu->arch.previous_host_tsc_for_scaling) ? 0 : 1;
+    }
+
+    if (host_tsc_since_last_rdtsc_handler_exit < HOST_CYCLES_TIMING_TEST_THRESHOLD) {
+        guest_tsc_increment = div_u64(delta_host_tsc, 20);
+
+        if (delta_host_tsc > 0 && guest_tsc_increment == 0) {
+            guest_tsc_increment = 1;
+        }
+    } else {
+        if (unlikely(vcpu->arch.host_calibrated_khz == 0 || vcpu->arch.host_calibrated_khz == 1)) {
+            printk_once(KERN_WARNING "KVM: TSC scaling fallback due to invalid host_calibrated_khz (%u).\n", vcpu->arch.host_calibrated_khz);
+            guest_tsc_increment = div_u64(delta_host_tsc, 10);
+        } else {
+            guest_tsc_increment = mul_u64_u32_div(delta_host_tsc,
+                                                  vcpu->arch.guest_target_khz,
+                                                  vcpu->arch.host_calibrated_khz);
+        }
+
+        if (delta_host_tsc > 0 && guest_tsc_increment == 0) {
+            guest_tsc_increment = 1;
+        }
+    }
+
+    vcpu->arch.guest_tsc_accumulator += guest_tsc_increment;
+    vcpu->arch.previous_host_tsc_for_scaling = current_host_tsc_at_entry;
+    
+    return vcpu->arch.guest_tsc_accumulator;
+}
+
+static int handle_rdtsc(struct kvm_vcpu *vcpu) {
+    u64 guest_tsc_value = get_scaled_guest_tsc(vcpu);
+
+    vcpu->arch.regs[VCPU_REGS_RAX] = guest_tsc_value & -1u;
+    vcpu->arch.regs[VCPU_REGS_RDX] = (guest_tsc_value >> 32) & -1u;
+
+	vcpu->arch.last_rdtsc_handler_exit_host_tsc = rdtsc();
+    return skip_emulated_instruction(vcpu);
+}
+
+static int handle_rdtscp(struct kvm_vcpu *vcpu) {
+    u64 guest_tsc_value = get_scaled_guest_tsc(vcpu);
+
+    vcpu->arch.regs[VCPU_REGS_RAX] = guest_tsc_value & 0xFFFFFFFF;
+    vcpu->arch.regs[VCPU_REGS_RDX] = (guest_tsc_value >> 32) & 0xFFFFFFFF;
+    vcpu->arch.regs[VCPU_REGS_RCX] = vcpu->vcpu_id;
+
+	vcpu->arch.last_rdtsc_handler_exit_host_tsc = rdtsc();
+    return skip_emulated_instruction(vcpu);
+}
+
+static int handle_umwait(struct kvm_vcpu *vcpu)
+{
+	if (print_once) {
+        printk("[handle_umwait] fake umwait vmx function is working\n");
+    }
+
+	return skip_emulated_instruction(vcpu);
+}
+
+static int handle_tpause(struct kvm_vcpu *vcpu)
+{
+	if (print_once) {
+        printk("[handle_tpause] fake tpause vmx function is working\n");
+    }
+	
+	return skip_emulated_instruction(vcpu);
+}
+
 /*
  * The exit handlers return 1 if the exit was handled fully and guest execution
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
@@ -6171,6 +6287,10 @@ static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_ENCLS]		      = handle_encls,
 	[EXIT_REASON_BUS_LOCK]                = handle_bus_lock_vmexit,
 	[EXIT_REASON_NOTIFY]		      = handle_notify,
+	[EXIT_REASON_RDTSC]                   = handle_rdtsc,
+	[EXIT_REASON_RDTSCP]                  = handle_rdtscp,
+	[EXIT_REASON_UMWAIT]                  = handle_umwait,
+	[EXIT_REASON_TPAUSE]				  = handle_tpause,
 };
 
 static const int kvm_vmx_max_exit_handlers =
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 951e44dc9d0e..8c22b72edfab 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -529,6 +529,7 @@ static inline u8 vmx_get_rvi(void)
 	 CPU_BASED_MONITOR_EXITING |					\
 	 CPU_BASED_INVLPG_EXITING |					\
 	 CPU_BASED_RDPMC_EXITING |					\
+	 CPU_BASED_RDTSC_EXITING |					\
 	 CPU_BASED_INTR_WINDOW_EXITING)
 
 #ifdef CONFIG_X86_64
@@ -542,8 +543,7 @@ static inline u8 vmx_get_rvi(void)
 #endif
 
 #define KVM_OPTIONAL_VMX_CPU_BASED_VM_EXEC_CONTROL			\
-	(CPU_BASED_RDTSC_EXITING |					\
-	 CPU_BASED_TPR_SHADOW |						\
+	(CPU_BASED_TPR_SHADOW |						\
 	 CPU_BASED_USE_IO_BITMAPS |					\
 	 CPU_BASED_MONITOR_TRAP_FLAG |					\
 	 CPU_BASED_USE_MSR_BITMAPS |					\
